Entropy is a scientific concept, as well as a measurable physical property,
that is most commonly associated with a state of disorder, randomness, or
uncertainty. The term and the concept are used in diverse fields, from
classical thermodynamics, where it was first recognized, to the microscopic
description of nature in statistical physics, and to the principles of
information theory. It has found far-ranging applications in chemistry and
physics, in biological systems and their relation to life, in cosmology,
economics, sociology, weather science, climate change, and information systems
including the transmission of information in telecommunication.[1]

The thermodynamic concept was referred to by Scottish scientist and engineer
William Rankine in 1850 with the names thermodynamic function and
heat-potential.[2] In 1865, German physicist Rudolf Clausius, one of the
leading founders of the field of thermodynamics, defined it as the quotient of
an infinitesimal amount of heat to the instantaneous temperature. He initially
described it as transformation-content, in German Verwandlungsinhalt, and later
coined the term entropy from a Greek word for transformation. Referring to
microscopic constitution and structure, in 1862, Clausius interpreted the
concept as meaning disgregation.[3]
